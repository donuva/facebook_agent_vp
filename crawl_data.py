# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
import os
import requests
import json
from datetime import datetime
from playwright.sync_api import sync_playwright
from playwright.sync_api._context_manager import PlaywrightContextManager

# Äá»‹nh nghÄ©a cÃ¡c háº±ng sá»‘ Ä‘Æ°á»ng dáº«n vÃ  URL cÆ¡ báº£n
SAVE_DIR = "cloned_site"
URL_BASE = "https://www.vpbank.com.vn/"

# Táº¡o thÆ° má»¥c lÆ°u káº¿t quáº£ theo thá»i gian crawl
timestamp = datetime.now()
date_str = timestamp.strftime("%Y%m%d_%H%M%S")
OUTPUT_DIR = os.path.join("crawl_data", date_str)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# HÃ m lÆ°u ná»™i dung vÄƒn báº£n vÃ o file
def save_content(content, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(content)

# HÃ m crawl ná»™i dung chi tiáº¿t cá»§a tá»«ng trang
def crawl_content(text: str, url: str, p: PlaywrightContextManager):
    browser = p.chromium.launch()
    page = browser.new_page()
    page.goto(url, wait_until="networkidle")

    results = {"title": text, "data": [], "documents": []}
    items = page.locator(".card-detail-content__body__item")
    count = items.count()

    for i in range(count):
        item = items.nth(i)
        try:
            title = item.get_attribute("id")
            if not title:
                continue
        except:
            continue

        # Náº¿u lÃ  pháº§n cÃ¢u há»i thÆ°á»ng gáº·p (FAQ)
        if title == "faqs":
            faq_list = []
            faq_cards = item.locator(".basic-accordion__card")
            faq_count = faq_cards.count()

            for j in range(faq_count):
                card = faq_cards.nth(j)
                try:
                    # Láº¥y cÃ¢u há»i
                    question = (
                        card.locator(".basic-accordion__card__header__text a")
                        .inner_text()
                        .strip()
                    )
                    # Láº¥y cÃ¢u tráº£ lá»i
                    answer = (
                        card.locator(".basic-accordion__card__collapse__content")
                        .inner_text()
                        .strip()
                    )
                    if question and answer:
                        faq_list.append({"question": question, "answer": answer})
                except:
                    continue

            if faq_list:
                results["data"].append({title: faq_list})

        # Náº¿u lÃ  pháº§n tÃ i liá»‡u (documents)
        elif title == "documents":
            documents_info = []
            doc_links = item.locator(".card-detail-content__body__item__right a")
            doc_count = doc_links.count()

            for j in range(doc_count):
                try:
                    link = doc_links.nth(j)
                    href = link.get_attribute("href")
                    filename = os.path.basename(href)
                    pdf_path = os.path.join(OUTPUT_DIR, filename)
                    # Náº¿u lÃ  file PDF thÃ¬ táº£i vá»
                    if filename.endswith(".pdf"):
                        r = requests.get(URL_BASE + href)
                        with open(pdf_path, "wb") as f:
                            f.write(r.content)
                            documents_info.append(
                                {
                                    "title": filename,
                                    "url": href,
                                    "local_file": filename,
                                }
                            )
                except Exception as e:
                    print(f"âŒ Lá»—i khi xá»­ lÃ½ document link: {e}")
                    continue

            # Náº¿u cÃ³ ná»™i dung vÄƒn báº£n thÃ¬ lÆ°u vÃ o results
            if content_texts:
                results["data"].append({title: documents_info})

        # Náº¿u lÃ  pháº§n ná»™i dung vÄƒn báº£n thÃ´ng thÆ°á»ng
        else:
            content_texts = []
            paragraphs = item.locator(".card-detail-content__body__item__right li")
            li_count = paragraphs.count()

            if li_count:
                for j in range(li_count):
                    try:
                        text_p = paragraphs.nth(j).inner_text().strip()
                        if text_p:
                            content_texts.append(text_p)
                    except:
                        continue
            else:
                try:
                    content = (
                        item.locator(".card-detail-content__body__item__right")
                        .inner_text()
                        .strip()
                    )
                    if content:
                        content_texts.append(content)
                except:
                    pass

            if content_texts:
                results["data"].append({title: content_texts})

    browser.close()
    return results

# HÃ m crawl tá»«ng trang chuyÃªn má»¥c cá»¥ thá»ƒ
def crawl_page(text: str, url: str, p: PlaywrightContextManager):
    try:
        browser = p.chromium.launch()
        page = browser.new_page()
        page.goto(URL_BASE + url, wait_until="networkidle")

        item_links = page.locator(".card-item__description")
        count = item_links.count()

        if count == 0:
            print("âŒ KhÃ´ng tÃ¬m tháº¥y pháº§n tá»­ nÃ o.")
            return

        all_results = []

        for i in range(count):
            item = item_links.nth(i)

            a_tag = item.locator("h3.name a")
            name = a_tag.inner_text()
            href = a_tag.get_attribute("href")

            # Gá»i hÃ m crawl chi tiáº¿t
            results = crawl_content(name, href, p)
            if results:
                all_results.append(results)

        # LÆ°u káº¿t quáº£ vÃ o file JSON
        file_name = f"{OUTPUT_DIR}/{text}.json"
        with open(file_name, "w", encoding="utf-8") as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)

        print(f"âœ… ÄÃ£ lÆ°u vÃ o file: {file_name}")
    except Exception as e:
        print(f"âŒ ÄÃ£ xáº£y ra lá»—i trong quÃ¡ trÃ¬nh crawl trang: {e}")
    finally:
        if browser:
            browser.close()

# Cháº¡y chÃ­nh - báº¯t Ä‘áº§u tá»« sitemap Ä‘á»ƒ láº¥y cÃ¡c liÃªn káº¿t trang con
with sync_playwright() as p:
    browser = p.chromium.launch()

    # Má»Ÿ sitemap Ä‘á»ƒ láº¥y danh sÃ¡ch cÃ¡c chuyÃªn má»¥c ngÆ°á»i dÃ¹ng
    page = browser.new_page()
    page.goto(URL_BASE + "sitemap", wait_until="networkidle")

    category_locator = page.locator(".category")

    sitemap = category_locator.nth(0)  # cÃ³ thá»ƒ lÃ  pháº§n tá»•ng thá»ƒ
    user_partition = category_locator.nth(1)  # pháº§n cho ngÆ°á»i dÃ¹ng cÃ¡ nhÃ¢n

    # Láº¥y táº¥t cáº£ cÃ¡c liÃªn káº¿t trong chuyÃªn má»¥c ngÆ°á»i dÃ¹ng, loáº¡i bá» cÃ¡c tháº» tiÃªu Ä‘á»
    links = user_partition.locator(":not(h1):not(h2):not(h3):not(h4):not(h5):not(h6) a")
    print(links.count())

    for j in range(links.count()):
        print(j)
        href = links.nth(j).get_attribute("href")
        text = links.nth(j).inner_text()
        print(f"ğŸ”— {text} --> {href}")

        # Crawl tá»«ng trang chuyÃªn má»¥c
        crawl_page(text, href, p)
